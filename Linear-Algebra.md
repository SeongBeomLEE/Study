# 선형대수학
## 선형대수학이란
- 선형대수는 간단하게 말하면 행렬과 벡터를 배우는 수학의 한 분야라고 말할 수 있음
- 선형대수는 어떤 함수가 선형함수일 때, 그 함수의 성질을 배우는 학문임
- 선형함수라는 것은 선형성을 만족하는 함수를 의미함

## 선형성과 비선형성
- 중첩과 동질성을 만족할 때 우리는 선형성을 가진다고 하며, 이 선형성을 만족하는 함수가 선형 함수라고 부름
- 중첩이란 f를 함수라고 할 때, f(x1 + x2) = f(x1) + f(x2)가 성립될 때를 말함
- 동질성이란 f를 함수라고 할 때, f(ax) = af(x)가 성립될 때를 말함
- 에를 들어 y = 2x라는 함수가 있을 때 이 값은 중첩과 동질성을 만족하여 해당 함수는 선형함수라고 할 수 있음
- 반대로 시그모이드 함수의 경우 sigmoid(10) = sigmoid(5) + sigmoid(5) 처럼 두 값은 서로 같지 않음, 따라서 시그모이드 함수는 중첩과 동질성을 만족하지 않기 때문에 비선형함수라고 하는 것임
- 그러면 왜 딥러닝은 비선형성을 강조하는 것일까? 이 부분에 대해서 생각해보면 딥러닝 자체로는 행렬과 행렬의 곱이기 때문에 여기 까지는 선형 변환이라고 할 수 있다. 근데 이 행렬을 무수히 많이 쌓아도 결국엔 선형 변황이기 때문에 데이터 간의 거리는 동일하게 유지 될 것이다. 하지만 여기에 활성화 함수로 비선형함수, 즉 비선혓성을 모델에 추가하게 되면 데이터 간의 거리에 변화가 생길 것이다. 즉 비선형함수는 데이터 간의 거리, 즉 데이터를 맵핑 시키는 공간에 위치 정보를 변화시켜준다고 볼 수 있다. 이 위치 정보의 변화는 곧 표현의 다양성이라고 볼 수 있으며, 데이터를 위치 공간에 새롭게 맵핑해줌으로써 우리는 선형으로 분류할 수 없던 데이터를 선형으로 분류할 수 있는 공간에 맵핑됨에 따라 분류가 가능하게 되는 것이다. 이렇게 비선형 변환을 바탕으로 선형으로 분류할 수 없던 데이터를 선형으로 분류할 수 있게 만들어주는 알고리즘이 딥러닝이라고 할 수 있고, 이러한 방법은 Represention Learning이라 부른다.

## 벡터, 벡터의 기본 연산
- 벡터란 크기와 방향으로 정의된 값, 숫자를 나열한 리스트, 공간 상의 하나의 점, 벡터 공간의 하나의 원소, 원점으로부터의 상대적 위치 등으로 표현할 수 있음
- 벡터의 양수의 스칼라를 곱했을 때 1 이상의 값이라면 그 크기가 커지고, 1 미만의 값이라면 그 크기가 작아짐, 반대로 음수를 곱하면 벡터의 방향이 반대가 됨 
- 벡터의 합은 단순히 동일한 위치의 원소를 서로 더해준 것이며, 다른 벡터로 부터의 상대적 위치 이동이라고 생각할 수 있음
- 벡터의 성분 곱은 두 벡터의 차원 중 서로 같은 방향을 바라보는 차원을 크게 만드는 과정이라고 볼 수 있음

## 선형 결합
- 선형 결합은 어려운게 아니고 단순히 위에 설명한 상수배와 벡터 간의 합을 한번에 조합해 표현하는 것을 의미함
- 벡터 간의 선형 결합이 어떤 벡터공간 전체에 대응된다는 개념을 공간 생성(span)이라고 하며, 이는 행렬 곱과 연립방정식의 해를 얻는 과정에 대한 새로운 관점을 제시해줄 아주 중요한 단서가 됨

## 선형독립과 선형종속 그리고 다중공선성
- 선형독립과 선형종속은 어떠한 벡터를 다른 벡터들의 선형결합으로 쓸 수 있는지 여부를 말함
- 주어진 하나의 벡터를 다른 벡터 들의 선형결합으로 쓸 수 있으면 선형종속이며, 쓸 수 없다면 선형 독립임
- 이 선형독립과 선형종속의 관계가 곧 다중 공선성의 개념에 적용되어, 변수간의 다중공선성 발생 여부를 확인할 수 있음
- 다중공선성이란 여러 개의 벡터가 선형적으로 같은 방향을 가리키고 있는 문제라고 할 수 있음, 즉 선형 종속인 관계의 벡터가 존재한다는 소리임(이것은 또 변수간의 상관계수가 높다는 것을 의미함)
- 여기서 여러 개의 독립 변수가 다중공선성에 의하여 같은 방향을 가리키게 되면, 변수간의 독립성을 가정하는 머신러닝 대부분의 기법의 가정을 어긋나게 됨
- 따라서 이런 다중공선성을 해결하기 위해서는 PCA를 수행하여 고유 벡터들만 남기거나(선형 독립인 벡터들만 사용, 그런데 모델의 설명력을 고려해야 함), 상관 계수가 높은 변수를 제거할 수 있음(상관관계가 0.7 이상)
- 결국 다중공선성이 발생한다는 것은 하나의 변수가 설명할 수 있는 부분이 다른 변수들과 겹치게 되어 모든 변수를 효과적으로 활용할 수 없다는 것을 의미함(회귀 계수 추정이 불안정 해짐)

## L1 norm과 L2 norm + L1 Regularization과 L2 Regularization
- 벡터의 norm은 원점에서부터의 거리를 의미함
- 벡터의 norm은 크게 L1 norm과 L2 norm으로 나뉘어짐
- L1 norm은 각 성분 변화량의 절대값을 모두 더하는 방식으로 맨하튼 거리를 의미함
- L2 norm은 각 성분 변화량의 제곱값을 모두 더하는 방식으로 유클리디안 거리를 의미함
- L1 norm과 L2 norm은 계산 방법의 차이에 의하여 최단 거리로의 이동하는 방법의 개수가 다름
- L1 norm은 만약에 [4, 4]로 이동한다고 했을 때 최단 거리는 8 이며, 최단 거리를 8로 가지는 경우의 수는 매우 많음((4, 0) -> (4, 4) / (0, 4) -> (4, 4) 등으로 최단 거리를 8로 하면서 [4, 4]로 이동할 수 있음)
- L2 norm은 만약에 [4, 4]로 이동한다고 했을 때 최단 거리는 32**0.5며 이러한 최단 거리를 가지는 경우의 수는 단 1개 밖에 없음(unique short path)
- 단순히 대각선으로 가로질러 가는 것 방법뿐임
- L1 norm과 L2 norm은 계산 방법의 차이에 의하여 공간 상의 원을 표현하는 방식의 차이를 가져옴, L1 norm은 마름모가 원으로 표현되고, L2 norm은 원으로 표현됨
- 공간 상의 원을 표현하는 방식의 차이에 의하여 L1 norm은 변수 선택의 기능으로 활용될 수 있음
- 우리가 L1과 L2 규제를 변수에 가한다고 했을 때, 우리가 최적화 해야하는 목적 함수가 총 2개가 됨
- 바로 모델의 예측 값에 위한 loss와 가중치에 대한 loss(Regularization)임
- 그러면 단순히 규제가 없다고 하면 우리는 모델의 예측 값에 위한 loss를 최소화 시키는 방향으로만 학습 시키면 됨
- 그런데 만약에 규제가 있다고 히면 모델의 예측 값에 위한 loss만을 최소화 시키면 가중치에 대한 loss가 올라갈 수도 있을 것임
- 따라서 규제가 있으면 모델의 예측 값에 위한 loss와 가중치에 대한 loss를 모두 최소화 시킬 수 있는 방향으로 모델이 학습되게 됨(즉 목적 함수 상의 최소점이 다르다는 것)
- 여기서 모델의 예측 값에 위한 loss를 원이라고 표현하고 가중치에 대한 loss를 원이라고 표현한다면 두 원 사이의 접점이 곧 목적 함수 상의 최소점일 것임
- 그런데 우리는 L1 norm과 L2 norm의 계산 방식에 차이에 의하여 공간 상의 원을 표현하는 방식의 차이가 존재한다는 것을 암
- L1 norm은 마름모 꼴로, L2 norm은 원 꼴로 표현됨
- L1 norm은 마름모 꼴로 표현되어 예측 값에 위한 loss의 지점과 만나는 접점이 마름모의 꼭짓점 즉, 어떠한 W가 0에 수렴하는 위치일 가능성이 높아짐
- 반대로 L2 norm은 원 꼴로 표현되어 예측 값에 위한 loss의 지점과 만나는 접점이 원의 꼭짓점(0,1 / 1,0 등의 지점) 보다는 평면 상의 점과 만날 가능성이 높아서 즉, 어떠한 W가 0에 수렴하는 위치가 아닐 가능성이 높아짐
- 이러한 이유 때문에 L1 norm은 L2 norm보다 어떠한 W의 값이 0에 수렴할 가능성이 높아져서, 변수 선택의 기능으로 활용될 수 있는 것임

## 벡터의 내적
- 벡터의 내적은 정사영된 벡터의 길이와 관련이 있음
- 벡터의 내적이란 같은 크기의 벡터를 동일한 원소의 위치에 값 끼리 서로 곱하여 합한 것이라고 할 수 있음
- 이에 내적 값은 두 벡터의 차원 중 서로 같은 방향을 바라 보는 값이 많을 수록 그 값이 커지게 됨
- 따라서 벡터의 내적을 이용하면 두 벡터가 서로 얼마나 유사한지를 판단할 수 있음

## 내적 유사도와 코사인 유사도
- 우리는 앞서 L2 norm과 벡터의 내적을 배웠기 때문에 코사인 유사도를 계산할 수 있음
- 앞서 벡터의 내적을 이용하면 두 벡터가 서로 얼마나 유사한지를 판단할 수 있다고 했으며, 이를 내적 유사도라고 부름
- 그런데 내적 유사도의 경우 하나의 벡터의 값이 매우 크면 그 값의 영향을 크게 받아 작은 벡터와 곱해지더라도 그 값이 매우 커진다는 단점이 존재함
- 이에 벡터의 크기에 상관 없이 표준화된 벡터 간의 유사도 계산법이 필요하고 그것이 바로 코사인 유사도임
- 코사인 유사도는 L2 norm과 벡터의 내적을 이용하여 계산되면, 단순히 벡터의 방향만을 고려한 유사도 계산 방법임
- 정리하면 내적 유사도는 벡터의 방향과 크기를 모두 고려하고, 코사인 유사도는 벡터의 방향만 고려함
- 따라서 벡터를 정규화 한후 내적 유사도를 계산하면 코사인 유사도와 동일함
- 그런데 역서 벡터의 크기란 어떻게 보면 가장 인기있는 벡터라고 할 수 있으며, 만약에 가장 인기있는 벡터를 고려학 싶다면 내적 유사도를 쓰면 되는 것이고, 단순히 방향만이 중요하다면 코사인 유사도를 쓰면 되는 것임
- 위와 같은 이유 때문에 Word2Vec 유사도 계산 시에도 코사인 유사도가 쓰이는 것으로 암
- 왜냐하면 빈도 수가 많은 단어, 다시 말하면 인기도가 높은 단어일 수록 벡터의 크기가 커지기 때문에 그 값에 크게 영향을 받는 유사도 계산이 되기 때문임(빈도 수가 많은 단어가 무조건 중요한 단어라는 보장이 없음)

## 고유값과 고유벡터
- 행렬은 벡터를 변환시켜서 다른 벡터를 출력 시키는 역할을 함

- 그런데 특정한 벡터와 행렬을 선형 변환을 취해주었을 때, 크기만 바뀌고 방향이 바뀌지 않는 경우가 존재함

- 이런 벡터를 고유 벡터라 하며, 고유 벡터의 크기에 영향을 주는 값을 고유값이라고 함

- 즉, 선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유 벡터라 하고, 그 상수배 값을 고유값이라 함

- 여기서 고유벡터는 선형변환 A에 의해 방향은 보존되고 스케일만 변환되는 방향 벡터를 의미하고, 고유값은 고유벡터의 스케일 정도를 나타낸 값이라고 할 수 있음

- 따라서 고유벡터는 행렬이 벡터에 작용하는 주축의 방향을 나타낸다고 할 수 있고, 그 방향의 크기를 고유값이 나타낸다고 할 수 있음

## EVD
- Eigen-Value Decomposition는 정방 행렬인 A를 자신의 고유벡터들을 열벡터로 하는 행렬과 고윳값을 대각원소로 하는 행렬의 곱으로 분해하는 것을 말함

- 따라서 각 고유 벡터들은 서로 수직일 것임(선형 독립)

## SVD
- Singular Value Decomposition도 고유값 분해 처럼 행렬을 대각화하는 방법임, 그런데 특이값 분해는 고유값 분해와 달리 정방행렬이 아니어도 분해가 가능함

- 행렬의 특이값(singular value)이란 이 행렬로 표현되는 선형변환의 스케일 변환을 나타내는 값으로 해석할 수 있음

- 고유값분해(eigendecomposition)에서 나오는 고유값(eigenvalue)과 비교해 보면 고유값은 변환에 의해 불변인 방향벡터(-> 고유벡터)에 대한 스케일 factor이고, 특이값은 변환 자체의 스케일 factor로 볼 수 있음

## PCA
- PCA를 한마디로 말하면 입력 데이터들의 공분산 행렬(covariance matrix)에 대한 고유값분해(eigendecomposition)로 볼 수 있음
- 공분산 행렬은 일종의 행렬로써, 데이터의 구조를 설명해주며, 특히 특징 쌍(feature pairs)들의 변동이 얼마나 닮았는가(다른 말로는 얼마만큼이나 함께 변하는가)를 설명해주는 행렬임
- 이 때 나오는 고유벡터가 주성분 벡터로서 데이터의 분포에서 분산이 큰 방향을 나타내고, 대응되는 고유값(eigenvalue)이 그 분산의 크기를 나타냄
- 여기서 주성분이라 함은 그 방향으로 데이터들의 분산이 가장 큰 방향벡터를 의미
- 이에 고유값이 큰 순서대로 고유벡터를 나열하면 데이터를 가장 잘 설명할 수 있는 벡터들이 모인 것이라고 할 수 있음
- 따라서 PCA는 여러 데이터들이 모여 하나의 분포를 이룰 때 이 분포의 주 성분을 분석해주는 방법이며, n개의 주성분 벡터가 반한됨

## PCA를 이용하여 차원축소를 할 수 있는 이유
- 우선 PCA를 이용해 차원 축소를 하는 방법에 대해서 생각을 해보자
- 우선 X 데이터의 공분산 행렬을 구함
- 공분산 행렬에 대하여 고유값분해를 진행
- 고유벡터를 고유값 기준으로 나열
- X와 고유벡터를 내적하여 n개의 주성분 벡터를 반환
- 우선 공분산 행렬이란 현재 변수들이 서로 얼마나 비슷한지를 나타내는 행렬을 의미함
- 따라서 공분산 행렬의 고유값분해로 반환된 고유벡터들은 데이터의 분포에서 분산이 큰 방향, 즉 해당 변수들을 가장 잘 설명할 수 있는 벡터를 의미함
- 이 X와 해당 고유벡터를 내적함으로써 우리는 해당 데이터를 가장 잘 설명할 수 있는 공간으로 새롭게 데이터를 맵핑시키는 것임
- 또한 고유벡터는 선형 독립이며 행렬과 벡터의 곱은 선형 독립인 벡터에 정사영 시키는 것이라고 볼 수 있어 새롭게 구한 차원들도 서로 선형 독립의 관게를 가짐
- 따라서 PCA를 이용하여 차원 축소를 할 수 있는 것

## 행렬곱
- 행렬은 벡터를 원소로 가지는 2차원 배열임
- 행렬은 벡터를 원소로 가지기 때문에 여러 개의 점을 나타냄
- 행렬은 하나의 함수로 생각할 수 있음(연립방정식을 행렬로 표현할 수 있는 것처럼)
- https://angeloyeo.github.io/2020/09/08/matrix_multiplication.html
- https://angeloyeo.github.io/2020/09/09/row_vector_and_inner_product.html
- 위 내용들을 바탕으로 벡터의 내적도 수정하기

## 기저 벡터
- 기저란 좌표계에서 각 좌표에 존재한 그 좌표가 가리키는 방향으로의 단위 벡터를 의미함

## 참고 자료
- https://angeloyeo.github.io/
- https://darkpgmr.tistory.com/
- https://blog.daum.net/eigenvalue/10856412
- https://blog.naver.com/PostList.naver?blogId=cindyvelyn&from=postList&categoryNo=28&parentCategoryNo=28
- https://gosamy.tistory.com/category/%EC%84%A0%ED%98%95%EB%8C%80%EC%88%98%ED%95%99